<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Diffusion Transformer Implementation | Julien Rineau</title>
<meta name="keywords" content="">
<meta name="description" content="A PyTorch implementation of the Diffusion Transformer (DiT) model. With OpenAI&rsquo;s Sora demonstrating the power of DiTs for multidimensional tasks, they represents a stable and efficient approach any diffusion task (vision, audio, robotics etc..). This implementation provides a clean, modular codebase to extend DiT for various generative applications.
Code Repository
Architecture

Implementation details

Firstlayer: A Python class that initializes the input processing. It includes a Patchify module to convert images into patches, a learnable positional embedding, and separate embedding layers for timesteps and class labels. The forward method combines these elements to prepare the input for the DiT blocks.
b, c, h, w = x.shape
p = self.patch_size

# Reshape and permute to get patches
x = x.reshape(b, c, h // p, p, w // p, p)
x = x.permute(0, 2, 4, 3, 5, 1).contiguous()
x = x.view(b, -1, p * p * c)

# Project patches to embedding dimension
x = projection(x)

LayerNorm: The LayerNorm affine transformation (scaling and shifting) is provided by a adaLN_modulation module, which computes these parameters based on the conditioning vector. This allows the scaling and shifting to be input-dependent rather than fixed learnable parameters.
self.adaLN_modulation = nn.Sequential(
nn.SiLU(), nn.Linear(config.n_embd, 6 * config.n_embd, bias=True))

MLP: Transformers struggle with spatial coherence in image tasks, often producing &ldquo;patchy&rdquo; outputs. To address this, we add a depth-wise convolution to the FFN layer, as introduced in the LocalViT paper. This allows the model to mix information from nearby pixels efficiently, improving spatial awareness and output quality with minimal computational cost.
Modulate: Performs element-wise modulation of normalized inputs using computed shift and scale parameters. It&rsquo;s used in both DiTBlock and FinalLayer to apply adaptive normalization.
shift, scale = self.adaLN_modulation(c).chunk(6, dim=1)
x = modulate(self.norm_final(x), shift, scale)

SelfAttention: Multi-head self-attention module. It uses linear layers to project inputs into query, key, and value representations, applies scaled dot-product attention using PyTorch&rsquo;s functional API, and projects the output back to the original embedding dimension.
FinalLayer: It uses a LayerNorm without affine parameters, computes adaptive normalization parameters via the adaLN_modulation module, and applies a final linear projection to reshape the output to the desired dimensions.

Classifier-free guidance
\[ \hat{\varepsilon}_\theta(x_t, c) = \varepsilon_\theta(x_t, \emptyset) &#43; s \cdot (\varepsilon_\theta(x_t, c) - \varepsilon_\theta(x_t, \emptyset))\]
Where $c$ is the condition, $\emptyset$ is a null condition, and $s &gt; 1$ is the guidance scale.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/dit/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/dit/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
    integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
    integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous">
</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
    integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body, 
              {
                  delimiters: [
                      {left: '$$', right: '$$', display: true},
                      {left: '\\[', right: '\\]', display: true},
                      {left: '$', right: '$', display: false},
                      {left: '\\(', right: '\\)', display: false}
                  ]
              }
    );"></script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Julien Rineau (Alt + H)">Julien Rineau</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Diffusion Transformer Implementation
    </h1>
    <div class="post-meta"><span title='2024-05-17 00:00:00 +0000 UTC'>May 17, 2024</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#architecture" aria-label="Architecture">Architecture</a><ul>
                        
                <li>
                    <a href="#implementation-details" aria-label="Implementation details">Implementation details</a></li>
                <li>
                    <a href="#classifier-free-guidance" aria-label="Classifier-free guidance">Classifier-free guidance</a></li></ul>
                </li>
                <li>
                    <a href="#dataset" aria-label="Dataset">Dataset</a></li>
                <li>
                    <a href="#variational-autoencoder" aria-label="Variational Autoencoder">Variational Autoencoder</a></li>
                <li>
                    <a href="#simplified-diffusion-model-formulation" aria-label="Simplified Diffusion Model Formulation">Simplified Diffusion Model Formulation</a><ul>
                        
                <li>
                    <a href="#noise-schedule" aria-label="Noise Schedule">Noise Schedule</a></li>
                <li>
                    <a href="#forward-process" aria-label="Forward Process">Forward Process</a></li>
                <li>
                    <a href="#reverse-process" aria-label="Reverse Process">Reverse Process</a></li>
                <li>
                    <a href="#training-objective" aria-label="Training Objective">Training Objective</a></li>
                <li>
                    <a href="#implementation-with-hfs-ddpmscheduler" aria-label="Implementation with HF&rsquo;s DDPMScheduler">Implementation with HF&rsquo;s DDPMScheduler</a></li></ul>
                </li>
                <li>
                    <a href="#results" aria-label="Results">Results</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>A PyTorch implementation of the Diffusion Transformer (DiT) model. With OpenAI&rsquo;s Sora demonstrating the power of DiTs for multidimensional tasks, they represents a stable and efficient approach any diffusion task (vision, audio, robotics etc..). This implementation provides a clean, modular codebase to extend DiT for various generative applications.</p>
<p><a href="https://github.com/JulienRineau/diffusion-transformer">Code Repository</a></p>
<h2 id="architecture">Architecture<a hidden class="anchor" aria-hidden="true" href="#architecture">#</a></h2>
<p><img alt="DiT Architecture" src="/img/dit/architecture.png"></p>
<h3 id="implementation-details">Implementation details<a hidden class="anchor" aria-hidden="true" href="#implementation-details">#</a></h3>
<ul>
<li><strong>Firstlayer:</strong> A Python class that initializes the input processing. It includes a <code>Patchify</code> module to convert images into patches, a learnable positional embedding, and separate embedding layers for timesteps and class labels. The forward method combines these elements to prepare the input for the DiT blocks.
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>b, c, h, w <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>p <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>patch_size
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Reshape and permute to get patches</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>reshape(b, c, h <span style="color:#f92672">//</span> p, p, w <span style="color:#f92672">//</span> p, p)
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>contiguous()
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(b, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, p <span style="color:#f92672">*</span> p <span style="color:#f92672">*</span> c)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Project patches to embedding dimension</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> projection(x)
</span></span></code></pre></div></li>
<li><strong>LayerNorm:</strong> The LayerNorm affine transformation (scaling and shifting) is provided by a <code>adaLN_modulation</code> module, which computes these parameters based on the conditioning vector. This allows the scaling and shifting to be input-dependent rather than fixed learnable parameters.
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>self<span style="color:#f92672">.</span>adaLN_modulation <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>nn<span style="color:#f92672">.</span>SiLU(), nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>n_embd, <span style="color:#ae81ff">6</span> <span style="color:#f92672">*</span> config<span style="color:#f92672">.</span>n_embd, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span></code></pre></div></li>
<li><strong>MLP:</strong> Transformers struggle with spatial coherence in image tasks, often producing &ldquo;patchy&rdquo; outputs. To address this, we add a depth-wise convolution to the FFN layer, as introduced in the <a href="https://arxiv.org/pdf/2104.05707">LocalViT paper</a>. This allows the model to mix information from nearby pixels efficiently, improving spatial awareness and output quality with minimal computational cost.</li>
<li><strong>Modulate:</strong> Performs element-wise modulation of normalized inputs using computed shift and scale parameters. It&rsquo;s used in both DiTBlock and FinalLayer to apply adaptive normalization.
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>shift, scale <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>adaLN_modulation(c)<span style="color:#f92672">.</span>chunk(<span style="color:#ae81ff">6</span>, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> modulate(self<span style="color:#f92672">.</span>norm_final(x), shift, scale)
</span></span></code></pre></div></li>
<li><strong>SelfAttention:</strong> Multi-head self-attention module. It uses linear layers to project inputs into query, key, and value representations, applies scaled dot-product attention using PyTorch&rsquo;s functional API, and projects the output back to the original embedding dimension.</li>
<li><strong>FinalLayer:</strong> It uses a LayerNorm without affine parameters, computes adaptive normalization parameters via the <code>adaLN_modulation</code> module, and applies a final linear projection to reshape the output to the desired dimensions.</li>
</ul>
<h3 id="classifier-free-guidance">Classifier-free guidance<a hidden class="anchor" aria-hidden="true" href="#classifier-free-guidance">#</a></h3>
\[ \hat{\varepsilon}_\theta(x_t, c) = \varepsilon_\theta(x_t, \emptyset) + s \cdot (\varepsilon_\theta(x_t, c) - \varepsilon_\theta(x_t, \emptyset))\]<p>
Where $c$ is the condition, $\emptyset$ is a null condition, and $s &gt; 1$ is the guidance scale.</p>
<p>While effective, this technique doubles computational cost. Our DiT implementation omits it, prioritizing computational efficiency for future robotics applications where real-time performance is crucial. By optimizing only $p_\theta(x_{t-1}|x_t,c)$, we balance generation quality with speed, better suiting time-sensitive robotic control tasks for example.</p>
<h2 id="dataset">Dataset<a hidden class="anchor" aria-hidden="true" href="#dataset">#</a></h2>
<p>To test the architecture, the DiT was trained on the <a href="https://huggingface.co/datasets/huggan/cats">Cat datasets</a> which contains 20k high-res images of cats. Only the uncontinioned diffusion can be tested because all samples are cats.</p>
<p><img alt="Dataset sample" src="/img/dit/image_stack_cat.jpg"></p>
<p>A simple formatting is applied to every images:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torchvision<span style="color:#f92672">.</span>transforms<span style="color:#f92672">.</span>Compose([
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>Resize((size, size)),
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>RandomHorizontalFlip(),
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>ToTensor(),
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>Normalize([<span style="color:#ae81ff">0.5</span>], [<span style="color:#ae81ff">0.5</span>]),])
</span></span></code></pre></div><h2 id="variational-autoencoder">Variational Autoencoder<a hidden class="anchor" aria-hidden="true" href="#variational-autoencoder">#</a></h2>
<p>the DiT is trained in a reduced 32x32x4 latent space to lighten the calculations. Each images is projected on the latent space using a the <a href="https://huggingface.co/stabilityai/sd-vae-ft-mse">Stable Diffusion VAE</a>:
<img alt="Dataset sample" src="/img/dit/vae_example_cat.png"></p>
<p>The Stable Diffusion VAE employs a crucial scaling factor of 0.18215, a legacy of its original training. This factor scales the latent representations during encoding (multiplication) and decoding (division), ensuring the latent space statistics match what the diffusion model expects. This scaling is essential for maintaining compatibility with pre-trained Stable Diffusion components and for achieving stable training and high-quality generation in DiT models.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">StableDiffusionVAE</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, model_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;stabilityai/sd-vae-ft-mse&#34;</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>scaling_factor <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.18215</span>
</span></span><span style="display:flex;"><span>    [<span style="color:#f92672">...</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">encode</span>(self, image):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>            latent <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>vae<span style="color:#f92672">.</span>encode(image)<span style="color:#f92672">.</span>latent_dist<span style="color:#f92672">.</span>sample()
</span></span><span style="display:flex;"><span>            latent <span style="color:#f92672">=</span> latent <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>scaling_factor 
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> latent
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">decode</span>(self, latent):
</span></span><span style="display:flex;"><span>        latent <span style="color:#f92672">=</span> latent <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>scaling_factor 
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>            image <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>vae<span style="color:#f92672">.</span>decode(latent)<span style="color:#f92672">.</span>sample
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> image
</span></span></code></pre></div><h2 id="simplified-diffusion-model-formulation">Simplified Diffusion Model Formulation<a hidden class="anchor" aria-hidden="true" href="#simplified-diffusion-model-formulation">#</a></h2>
<p>We implement a variance-preserving diffusion model using the <code>DDPMScheduler</code> from the Hugging Face <code>Diffusers</code> library, with the <code>squaredcos_cap_v2</code> beta schedule. This approach maintains the Gaussian nature of the diffusion process while leveraging optimized scheduling for improved performance.</p>
<h3 id="noise-schedule">Noise Schedule<a hidden class="anchor" aria-hidden="true" href="#noise-schedule">#</a></h3>
\[ \beta_t = \min\left(\frac{t}{T}\cdot 0.999, \sqrt{1 - \left(\cos\left(\frac{t/T + s}{1 + s}\cdot \frac{\pi}{2}\right)\right)^2}\right) \]<p>
where $T$ is the total number of timesteps and $s$ is a small constant (default 0.008).
This schedule provides a smooth progression of noise levels, starting slow, accelerating in the middle, and then slowing down again at the end. The min operation caps the maximum beta value to 0.999.</p>
<h3 id="forward-process">Forward Process<a hidden class="anchor" aria-hidden="true" href="#forward-process">#</a></h3>
\[ q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t)\mathbf{I}) \]\[ x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \varepsilon_{t}, \text{ where } \varepsilon_{t} \sim \mathcal{N}(0, \mathbf{I}) \]<p>
Here, $\bar{\alpha_{t}} = \prod_{s=1}^{t} (1 - \beta_s)$ is the cumulative product of noise scale factors, computed by the DDPMScheduler based on the beta schedule.</p>
<h3 id="reverse-process">Reverse Process<a hidden class="anchor" aria-hidden="true" href="#reverse-process">#</a></h3>
\[ p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 \mathbf{I}) \]\[ \mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}t}}\varepsilon_{\theta(x_t, t)}) \]<p>
The <code>DDPMScheduler</code> handles the computation of $\alpha_t$, $\beta_t$, and $\bar{\alpha}_t$ based on the <code>squaredcos_cap_v2</code> schedule, ensuring consistency between the forward and reverse processes.</p>
<h3 id="training-objective">Training Objective<a hidden class="anchor" aria-hidden="true" href="#training-objective">#</a></h3>
\[ \mathcal{L_{\text{simple}}}(\theta) = |\varepsilon_{\theta(x_t, t)} - \varepsilon_{t}|^2 \]<h3 id="implementation-with-hfs-ddpmscheduler">Implementation with HF&rsquo;s DDPMScheduler<a hidden class="anchor" aria-hidden="true" href="#implementation-with-hfs-ddpmscheduler">#</a></h3>
<p>In practice, using the DDPMScheduler simplifies our implementation:</p>
<ol>
<li>
<p><strong>Initialization</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> diffusers <span style="color:#f92672">import</span> DDPMScheduler
</span></span><span style="display:flex;"><span>scheduler <span style="color:#f92672">=</span> DDPMScheduler(num_train_timesteps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, beta_schedule<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;squaredcos_cap_v2&#34;</span>)
</span></span></code></pre></div></li>
<li>
<p><strong>Forward Process</strong>:
The scheduler handles noise addition:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>noisy_samples, noise <span style="color:#f92672">=</span> scheduler<span style="color:#f92672">.</span>add_noise(original_samples, noise, timesteps)
</span></span></code></pre></div></li>
<li>
<p><strong>Reverse Process</strong>:
The scheduler computes the denoised sample given the model&rsquo;s noise prediction:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>denoised <span style="color:#f92672">=</span> scheduler<span style="color:#f92672">.</span>step(model_output, timestep, noisy_samples)<span style="color:#f92672">.</span>prev_sample
</span></span></code></pre></div></li>
</ol>
<h2 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h2>
<p>The model was trained for for 22h a Nvidia A10 with the following hyperparameters:</p>
<ul>
<li><strong>Patch size:</strong> 2</li>
<li><strong>Number of DiT blocks:</strong> 12</li>
<li><strong>Number of head:</strong> 12</li>
<li><strong>Embedding size:</strong> 768</li>
<li><strong>Batch size:</strong> 64</li>
<li><strong>Learning rate:</strong> 1e-4</li>
</ul>
<p><img alt="Training log" src="/img/dit/training_log.png"></p>
<p>When stopped the model was still learning and its performances can stilll be improved by scalling its size and letting it learn longer. Here is a sampling example:
<img alt="Results" src="/img/dit/results2.png"></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Julien Rineau</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
