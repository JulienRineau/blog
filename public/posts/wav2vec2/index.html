<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=blog/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Wav2Vec 2.0 Implementation | Julien Rineau</title>
<meta name="keywords" content="">
<meta name="description" content="A Wav2Vec 2.0 implementation using PyTorch Lightning. This project aims to create a clean, modifiable building block for speech reco
gnition research. It uses common tools for optimized training and effective monitoring. The implementation includes code for model training, dataset preparation, and evaluation. This page also details the results of pretraining on the LibriSpeech dataset. Code Repository
Architecture My implementation closely follows the Wav2Vec 2.0 BASE model architecture:
768 embedding size 8 attention heads 12 transformer blocks 512 convolutional channels in the feature encoder 2 groups and 320 choices per group in the quantizer This configuration results in approximately 95M parameters.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/blog/posts/wav2vec2/">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/blog/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blog/posts/wav2vec2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
    integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
    integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous">
</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
    integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body, 
              {
                  delimiters: [
                      {left: '$$', right: '$$', display: true},
                      {left: '\\[', right: '\\]', display: true},
                      {left: '$', right: '$', display: false},
                      {left: '\\(', right: '\\)', display: false}
                  ]
              }
    );"></script>

</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/blog/" accesskey="h" title="Julien Rineau (Alt + H)">Julien Rineau</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Wav2Vec 2.0 Implementation
    </h1>
    <div class="post-meta"><span title='2024-05-12 00:00:00 +0000 UTC'>May 12, 2024</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#architecture" aria-label="Architecture">Architecture</a></li>
                <li>
                    <a href="#pretraining" aria-label="Pretraining">Pretraining</a><ul>
                        
                <li>
                    <a href="#optimization-techniques" aria-label="Optimization Techniques">Optimization Techniques</a></li>
                <li>
                    <a href="#dataset" aria-label="Dataset">Dataset</a></li>
                <li>
                    <a href="#quantization-and-loss" aria-label="Quantization and Loss">Quantization and Loss</a></li>
                <li>
                    <a href="#pretraining-process" aria-label="Pretraining Process">Pretraining Process</a></li>
                <li>
                    <a href="#model-parameters" aria-label="Model Parameters">Model Parameters</a></li></ul>
                </li>
                <li>
                    <a href="#results" aria-label="Results">Results</a></li>
                <li>
                    <a href="#fine-tuning" aria-label="Fine-tuning">Fine-tuning</a></li>
                <li>
                    <a href="#how-to-use-this-implementation" aria-label="How to Use This Implementation">How to Use This Implementation</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>A Wav2Vec 2.0 implementation using PyTorch Lightning. This project aims to create a clean, modifiable building block for speech reco</p>
<p>gnition research. It uses common tools for optimized training and effective monitoring. The implementation includes code for model training, dataset preparation, and evaluation. This page also details the results of pretraining on the LibriSpeech dataset.
Code Repository</p>
<h2 id="architecture">Architecture<a hidden class="anchor" aria-hidden="true" href="#architecture">#</a></h2>
<p><img loading="lazy" src="img/wav2vec2/architechture_paper.png" alt="Wav2Vec architechture"  />

My implementation closely follows the Wav2Vec 2.0 BASE model architecture:</p>
<ul>
<li>768 embedding size</li>
<li>8 attention heads</li>
<li>12 transformer blocks</li>
<li>512 convolutional channels in the feature encoder</li>
<li>2 groups and 320 choices per group in the quantizer</li>
</ul>
<p>This configuration results in approximately 95M parameters.
The model processes raw audio waveforms, first through a convolutional feature encoder, then applies quantization, and finally processes the representations through a transformer to produce contextualized representations.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Wav2Vec2Base(
</span></span><span style="display:flex;"><span>  (feature_encoder): FeatureEncoder(
</span></span><span style="display:flex;"><span>    (conv_layers): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">0</span>): Sequential(
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">0</span>): Conv1d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">512</span>, kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>,), stride<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>,), padding<span style="color:#f92672">=</span>(<span style="color:#ae81ff">4</span>,), bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">1</span>): GroupNorm(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">512</span>, eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">2</span>): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span><span style="color:#ae81ff">4</span>): <span style="color:#ae81ff">4</span> x Sequential(
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">0</span>): Conv1d(<span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">512</span>, kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">3</span>,), stride<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>,), padding<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>,), bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">1</span>): Identity()
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">2</span>): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">5</span><span style="color:#f92672">-</span><span style="color:#ae81ff">6</span>): <span style="color:#ae81ff">2</span> x Sequential(
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">0</span>): Conv1d(<span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">512</span>, kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>,), stride<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>,), bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">1</span>): Identity()
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">2</span>): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (feature_projection): <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>  (positional_embedding): <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>  (transformer): ModuleList(
</span></span><span style="display:flex;"><span>    (<span style="color:#ae81ff">0</span><span style="color:#f92672">-</span><span style="color:#ae81ff">11</span>): <span style="color:#ae81ff">12</span> x TransformerBlock(
</span></span><span style="display:flex;"><span>      (attn): SelfAttention(
</span></span><span style="display:flex;"><span>        (q_proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (k_proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (v_proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (out_proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (dropout): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (mlp): MLP <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>      (ln_1): LayerNorm((<span style="color:#ae81ff">768</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      (ln_2): LayerNorm((<span style="color:#ae81ff">768</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      (dropout): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (ln_f): LayerNorm((<span style="color:#ae81ff">768</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>  (dropout): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>  (quantizer): VectorQuantizer(
</span></span><span style="display:flex;"><span>    (projector): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">640</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (out_linear): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h2 id="pretraining">Pretraining<a hidden class="anchor" aria-hidden="true" href="#pretraining">#</a></h2>
<h3 id="optimization-techniques">Optimization Techniques<a hidden class="anchor" aria-hidden="true" href="#optimization-techniques">#</a></h3>
<p>The implementation incorporates several optimization techniques:</p>
<ul>
<li><strong>Mixed Precision:</strong> BF16 mixed precision for computational efficiency and reduced memory usage.</li>
<li><strong>Gradient Clipping:</strong> Set to 1.0 to prevent exploding gradients.</li>
<li><strong>DDP (Distributed Data Parallel):</strong> Enables parallel processing on multiple GPUs, significantly accelerating training times.</li>
<li><strong>Scheduled Learning Rate</strong>: Warmup and cosine annealing like in the GPT-3 paper
<img loading="lazy" src="/img/wav2vec2/learning_rate.png" alt="Wav2Vec architechture"  />
</li>
</ul>
<h3 id="dataset">Dataset<a hidden class="anchor" aria-hidden="true" href="#dataset">#</a></h3>
<p>For pretraining, we use the LibriSpeech dataset, which consists of about 960 hours of English speech. The dataset is processed in streaming mode to handle large amounts of data efficiently.</p>
<p>Example:
<figure >
    <audio controls preload="metadata">
        
        <source src="/blog/audio.wav" type="audio/mpeg">
    </audio>
    <figcaption>&#39;&#39;Chapter sixteen: I might have told you of the beginning of this liaison in a few lines, but I wanted you to see every step by which we came to agree to whatever Marguerite wished.&#39;&#39;</figcaption>
</figure></p>
<h3 id="quantization-and-loss">Quantization and Loss<a hidden class="anchor" aria-hidden="true" href="#quantization-and-loss">#</a></h3>
<p>The model uses a Vector Quantizer to discretize the latent representations. This quantization is crucial for the contrastive loss used in pretraining. The loss function combines a contrastive loss, which identifies the correct quantized representation among distractors, and a diversity loss, which encourages the use of the full codebook.</p>
<p><img loading="lazy" src="/img/wav2vec2/pq.webp" alt="Quantization"  />
</p>
<h3 id="pretraining-process">Pretraining Process<a hidden class="anchor" aria-hidden="true" href="#pretraining-process">#</a></h3>
<p>The pretraining process follows these steps:</p>
<ul>
<li>The raw audio is passed through the feature encoder.</li>
<li>A proportion of the encoded features are masked.</li>
<li>The model attempts to predict the correct quantized representations for the masked time steps.</li>
<li>The loss is computed based on the model&rsquo;s predictions and the actual quantized representations.</li>
</ul>
<h3 id="model-parameters">Model Parameters<a hidden class="anchor" aria-hidden="true" href="#model-parameters">#</a></h3>
<p>Key parameters of the model include:</p>
<ul>
<li>n_layer: 12 (transformer blocks)</li>
<li>n_head: 8 (attention heads)</li>
<li>n_embd: 768 (embedding dimension)</li>
<li>ffn_dim: 3072 (feed-forward network dimension)</li>
<li>max_seq_len: 1024</li>
<li>conv_channels: 512 (in the feature encoder)</li>
<li>conv_kernel_sizes: (10, 3, 3, 3, 3, 2, 2)</li>
<li>conv_strides: (5, 2, 2, 2, 2, 2, 2)</li>
<li>dropout: 0.1</li>
<li>pos_conv_kernel: 128 (for positional embeddings)</li>
<li>pos_conv_groups: 16 (for positional embeddings)</li>
</ul>
<h2 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h2>
<p>TBD..</p>
<h2 id="fine-tuning">Fine-tuning<a hidden class="anchor" aria-hidden="true" href="#fine-tuning">#</a></h2>
<p>While the current implementation focuses on pretraining, fine-tuning capabilities are planned for future updates. This will allow the model to be adapted for specific speech recognition tasks.</p>
<h2 id="how-to-use-this-implementation">How to Use This Implementation<a hidden class="anchor" aria-hidden="true" href="#how-to-use-this-implementation">#</a></h2>
<p>For detailed instructions on how to use this implementation, refer to the GitHub repository.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="http://localhost:1313/blog/">Julien Rineau</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
