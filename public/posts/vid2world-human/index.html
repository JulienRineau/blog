<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>World Models for Human Manipulation | Julien Rineau</title>
<meta name="keywords" content="">
<meta name="description" content="I trained a world model that predicts how humans manipulate objects from a single image and an action sequence.

  

Given the first frame and 16-step action sequence, the model predicts future manipulation frames.
The premise is simple: if you can accurately simulate what happens when a human performs an action, you don&rsquo;t need a physical robot to learn manipulation. A policy can explore thousands of candidate action sequences in imagination, evaluating outcomes before committing to real-world execution. The bottleneck shifts from expensive robot time to GPU compute.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/vid2world-human/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/vid2world-human/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"
    integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"
    integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous">
</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
    integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body, 
              {
                  delimiters: [
                      {left: '$$', right: '$$', display: true},
                      {left: '\\[', right: '\\]', display: true},
                      {left: '$', right: '$', display: false},
                      {left: '\\(', right: '\\)', display: false}
                  ]
              }
    );"></script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Julien Rineau (Alt + H)">Julien Rineau</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      World Models for Human Manipulation
    </h1>
    <div class="post-meta"><span title='2025-01-16 00:00:00 +0000 UTC'>January 16, 2025</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#the-core-insight" aria-label="The Core Insight">The Core Insight</a></li>
                <li>
                    <a href="#architecture" aria-label="Architecture">Architecture</a><ul>
                        
                <li>
                    <a href="#training" aria-label="Training">Training</a></li>
                <li>
                    <a href="#inference" aria-label="Inference">Inference</a></li></ul>
                </li>
                <li>
                    <a href="#method" aria-label="Method">Method</a></li>
                <li>
                    <a href="#adapting-to-human-manipulation" aria-label="Adapting to Human Manipulation">Adapting to Human Manipulation</a></li>
                <li>
                    <a href="#what-the-model-learned" aria-label="What the Model Learned">What the Model Learned</a></li>
                <li>
                    <a href="#whats-next" aria-label="What&rsquo;s Next">What&rsquo;s Next</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>I trained a world model that predicts how humans manipulate objects from a single image and an action sequence.</p>
<video autoplay loop muted playsinline style="width: 100%; max-width: 800px; display: block; margin: 0 auto;">
  <source src="/img/vid2world/comparison.mp4" type="video/mp4">
</video>
<p><em>Given the first frame and 16-step action sequence, the model predicts future manipulation frames.</em></p>
<p>The premise is simple: if you can accurately simulate what happens when a human performs an action, you don&rsquo;t need a physical robot to learn manipulation. A policy can explore thousands of candidate action sequences in imagination, evaluating outcomes before committing to real-world execution. The bottleneck shifts from expensive robot time to GPU compute.</p>
<p>This isn&rsquo;t just theoretical. 1X Technologies recently demonstrated that world models can drive real humanoid robots, using video prediction to learn tasks with minimal robot-specific data <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Their insight: video diffusion models pretrained on internet-scale data already understand how the world moves—physics, object permanence, hand-object interactions. The question becomes: can we steer that knowledge with actions?</p>
<h2 id="the-core-insight">The Core Insight<a hidden class="anchor" aria-hidden="true" href="#the-core-insight">#</a></h2>
<p>Standard video diffusion models generate frames that look plausible but ignore what you <em>want</em> to happen. They&rsquo;re storytellers, not simulators. To make them useful for robotics, I needed two modifications:</p>
<ol>
<li><strong>Causality</strong>: Frame 10 shouldn&rsquo;t influence frame 5. The model must respect the arrow of time.</li>
<li><strong>Action conditioning</strong>: The model must understand &ldquo;if I move my hand here, this happens.&rdquo;</li>
</ol>
<p>The first sounds trivial—just mask attention—but pretrained video models have bidirectional temporal convolutions baked in everywhere. The second requires the model to learn a new input modality (actions) while preserving its video generation capabilities.</p>
<h2 id="architecture">Architecture<a hidden class="anchor" aria-hidden="true" href="#architecture">#</a></h2>
<h3 id="training">Training<a hidden class="anchor" aria-hidden="true" href="#training">#</a></h3>
<p>During training, I have access to full video sequences with paired actions. The key trick: rather than applying uniform noise across all frames, each frame gets an independent noise level. Frame 3 might be nearly clean while frame 12 is heavily corrupted.</p>
<p>Why does this matter? At inference, the model generates autoregressively—it has clean past frames and must generate noisy future frames. By training with variable noise levels, the model learns to leverage clean context to reconstruct corrupted frames. Uniform noise would never teach this skill.</p>
<img src="/img/vid2world/training.svg" alt="Training architecture" style="max-width: 500px; display: block; margin: 0 auto;">
<h3 id="inference">Inference<a hidden class="anchor" aria-hidden="true" href="#inference">#</a></h3>
<p>At test time, I only have the first frame. Generation proceeds one frame at a time: encode the initial frame, generate frame 2 by denoising conditioned on frame 1, generate frame 3 conditioned on frames 1-2, and so on.</p>
<p>This is where causal attention pays off. Because the model never saw future frames during training, it learned to make predictions from past context alone. KV-caching stores attention keys and values from previously generated frames, so I don&rsquo;t recompute the entire sequence at each step.</p>
<img src="/img/vid2world/inference.svg" alt="Inference architecture" style="max-width: 500px; display: block; margin: 0 auto;">
<h2 id="method">Method<a hidden class="anchor" aria-hidden="true" href="#method">#</a></h2>
<p>The model predicts velocity $v$ rather than noise $\epsilon$—a reparameterization that provides more stable gradients across timesteps <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>: $v = \sqrt{\bar{\alpha}_t} \cdot \epsilon - \sqrt{1-\bar{\alpha}_t} \cdot x_0$.</p>
\[ \mathcal{L} = \mathbb{E}_{t_1,...,t_{16}} \left[ \sum_{i=1}^{16} \|v_\theta(x_{t_i}, t_i, a_i) - v_{\text{target},i}\|^2 \right] \]<p>To make actions actually matter, I use classifier-free guidance. During training, I randomly drop actions with 15% probability—but crucially, I drop them <em>per-frame</em> rather than per-sequence. This teaches the model fine-grained action-outcome relationships. At inference, I amplify action influence: $v_{\text{guided}} = v_{\text{uncond}} + s \cdot (v_{\text{cond}} - v_{\text{uncond}})$ where $s &gt; 1$.</p>
<p>The trickiest part was converting pretrained weights to causal. Video diffusion models like DynamiCrafter use symmetric temporal convolutions—a kernel that looks at frames before <em>and after</em> the current frame. Simply zeroing the future-looking weights destroys learned dynamics. Instead, I used an extrapolative transformation: for a kernel $[w_0, w_1, w_2]$, the causal version becomes $[0, w_0 - w_2, w_1 + 2w_2]$. This preserves the effective temporal receptive field while enforcing strict causality.</p>
<h2 id="adapting-to-human-manipulation">Adapting to Human Manipulation<a hidden class="anchor" aria-hidden="true" href="#adapting-to-human-manipulation">#</a></h2>
<p>Most world models target robot arms with 7-DOF action spaces. I wanted to model <em>human</em> bimanual manipulation—two hands working together on deformable objects. This required designing a new action representation.</p>
<p>Each hand contributes 10 dimensions: 3D position delta, 6D rotation (two columns of the rotation matrix—more stable than Euler angles or quaternions for learning), and gripper width. The full 20D action captures the coordinated motion of both hands.</p>
<table>
  <thead>
      <tr>
          <th>Gripper</th>
          <th>Dimensions</th>
          <th>Encoding</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Left</td>
          <td>10D</td>
          <td>3D position delta + 6D rotation + 1D width</td>
      </tr>
      <tr>
          <td>Right</td>
          <td>10D</td>
          <td>3D position delta + 6D rotation + 1D width</td>
      </tr>
  </tbody>
</table>
<p>I collected 5,248 episodes of bimanual t-shirt folding using VR teleoperation (details in the <a href="/posts/quest-teleoperation/">teleoperation post</a>). The fisheye camera provides a wide field of view that captures both hands throughout the manipulation. Training ran at 320×512 resolution with batch size 2, gradient accumulation, learning rate 1e-5, and FP16 mixed precision.</p>
<h2 id="what-the-model-learned">What the Model Learned<a hidden class="anchor" aria-hidden="true" href="#what-the-model-learned">#</a></h2>
<p>The model captures the broad strokes: hand trajectories follow commanded actions, cloth deforms plausibly, spatial relationships stay coherent across frames.</p>
<p>To verify it learned generalizable dynamics rather than memorizing trajectories, I ran an action cross-swap: take frame A with actions from episode B. The grid below shows ground truth (diagonal) versus swapped actions (off-diagonal)—same starting frame, different action sequences producing different outcomes.</p>
<div style="position: relative; max-width: 800px; margin: 0 auto;">
  <div id="carousel" style="display: flex; overflow-x: hidden; scroll-snap-type: x mandatory; scroll-behavior: smooth; border-radius: 8px;">
    <video style="flex: 0 0 100%; scroll-snap-align: start; width: 100%;" autoplay loop muted playsinline>
      <source src="/img/vid2world/crossswap4.mp4" type="video/mp4">
    </video>
    <video style="flex: 0 0 100%; scroll-snap-align: start; width: 100%;" autoplay loop muted playsinline>
      <source src="/img/vid2world/crossswap2.mp4" type="video/mp4">
    </video>
    <video style="flex: 0 0 100%; scroll-snap-align: start; width: 100%;" autoplay loop muted playsinline>
      <source src="/img/vid2world/crossswap3.mp4" type="video/mp4">
    </video>
  </div>
  <button onclick="document.getElementById('carousel').scrollBy({left: -document.getElementById('carousel').offsetWidth})" style="position: absolute; left: 10px; top: 50%; transform: translateY(-50%); background: rgba(0,0,0,0.5); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; font-size: 20px; cursor: pointer;">&#10094;</button>
  <button onclick="document.getElementById('carousel').scrollBy({left: document.getElementById('carousel').offsetWidth})" style="position: absolute; right: 10px; top: 50%; transform: translateY(-50%); background: rgba(0,0,0,0.5); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; font-size: 20px; cursor: pointer;">&#10095;</button>
  <div id="carousel-dots" style="display: flex; justify-content: center; gap: 8px; margin-top: 10px;"></div>
</div>
<script>document.addEventListener("DOMContentLoaded",function(){var c=document.getElementById("carousel"),d=document.getElementById("carousel-dots");if(c&&d){var v=c.querySelectorAll("video");for(var i=0;i<v.length;i++){var dot=document.createElement("button");dot.style.cssText="width:10px;height:10px;border-radius:50%;border:none;background:#666;cursor:pointer";dot.setAttribute("data-i",i);dot.onclick=function(){c.scrollTo({left:c.offsetWidth*this.getAttribute("data-i")})};d.appendChild(dot)}function u(){var idx=Math.round(c.scrollLeft/c.offsetWidth);var dots=d.querySelectorAll("button");for(var j=0;j<dots.length;j++){dots[j].style.background=j===idx?"#fff":"#666"}}c.addEventListener("scroll",u);u()}});</script>
<p>This tracks with findings from 1X: video prediction quality correlates with downstream task success <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. If the world model can&rsquo;t accurately predict what happens when you grasp a shirt corner, a policy trained on its rollouts will fail at the real task. Visual fidelity isn&rsquo;t vanity—it&rsquo;s a proxy for physical understanding.</p>
<p>Current limitations reveal what&rsquo;s still hard:</p>
<ul>
<li><strong>Fine details decay</strong>: Fingers blur, cloth texture simplifies over longer horizons. The model takes shortcuts when it can.</li>
<li><strong>Complex dynamics</strong>: Cloth folding involves self-collision, layering, contact transitions. The model sometimes produces physically impossible configurations.</li>
<li><strong>Depth ambiguity</strong>: Like 1X&rsquo;s monocular system, a single fisheye camera provides weak 3D grounding. The model sometimes confuses depth ordering when hands cross.</li>
<li><strong>Large actions</strong>: Big displacements cause hallucination—the model hasn&rsquo;t seen enough extreme motions to generalize.</li>
</ul>
<h2 id="whats-next">What&rsquo;s Next<a hidden class="anchor" aria-hidden="true" href="#whats-next">#</a></h2>
<p>This world model is infrastructure for the real goal: learning manipulation policies without expensive robot rollouts. The next steps:</p>
<ul>
<li><strong>Inverse dynamics grounding</strong>: Following 1X&rsquo;s architecture, add an inverse dynamics model that extracts action sequences from generated frames. This bridges visual prediction to actionable control.</li>
<li><strong>Model-based policy learning</strong>: Train diffusion policies that plan in imagination, using the world model as a simulator.</li>
<li><strong>Longer horizons</strong>: Current 16-frame prediction isn&rsquo;t enough for complex tasks. Hierarchical action abstraction or best-of-N sampling at inference could extend temporal reach.</li>
</ul>
<p>The vision: collect human demonstrations once, train a world model, then train thousands of policies in simulation. Real robot time becomes validation, not training.</p>
<hr>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>1X Technologies demonstrated world models driving real humanoid robots with minimal robot-specific data. See <a href="https://www.1x.tech/discover/world-model-self-learning">World Model for Self-Learning</a> (1X, 2025).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>This work builds on video diffusion techniques for world modeling. See <a href="https://arxiv.org/abs/2505.14357">Vid2World: Crafting Video Diffusion Models to Interactive World Models</a> (Chen et al., 2025). <a href="https://knightnemo.github.io/vid2world/">Project page</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Julien Rineau</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
