<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Julien Rineau</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Julien Rineau</description>
    <generator>Hugo -- 0.141.0</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>World Models for Human Manipulation</title>
      <link>http://localhost:1313/posts/vid2world-human/</link>
      <pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/vid2world-human/</guid>
      <description>&lt;p&gt;I trained a world model that predicts how humans manipulate objects from a single image and an action sequence.&lt;/p&gt;
&lt;video autoplay loop muted playsinline style=&#34;width: 100%; max-width: 800px; display: block; margin: 0 auto;&#34;&gt;
  &lt;source src=&#34;http://localhost:1313/img/vid2world/comparison.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;&lt;em&gt;Given the first frame and 16-step action sequence, the model predicts future manipulation frames.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The premise is simple: if you can accurately simulate what happens when a human performs an action, you don&amp;rsquo;t need a physical robot to learn manipulation. A policy can explore thousands of candidate action sequences in imagination, evaluating outcomes before committing to real-world execution. The bottleneck shifts from expensive robot time to GPU compute.&lt;/p&gt;</description>
    </item>
    <item>
      <title>VR Teleoperation for Bimanual Robots</title>
      <link>http://localhost:1313/posts/quest-teleoperation/</link>
      <pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/quest-teleoperation/</guid>
      <description>&lt;p&gt;I built a system that lets you control a bimanual robot using a Meta Quest headset. Put on the headset, grab the controllers, and a Trossen AI Mobile robot mirrors your arm movements in real-time through inverse kinematics. The code is available on request.&lt;/p&gt;
&lt;p&gt;[VIDEO_REAL_ROBOT_PLACEHOLDER: Full teleoperation demo - controlling the real robot]&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;The system bridges consumer VR hardware to research-grade robotics through a streaming pipeline optimized for low latency.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Diffusion Transformer Implementation</title>
      <link>http://localhost:1313/posts/dit/</link>
      <pubDate>Fri, 17 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/dit/</guid>
      <description>&lt;p&gt;A PyTorch implementation of the Diffusion Transformer (DiT) model. With OpenAI&amp;rsquo;s Sora demonstrating the power of DiTs for multidimensional tasks, they represents a stable and efficient approach any diffusion task (vision, audio, robotics etc..). This implementation provides a clean, modular codebase to extend DiT for various generative applications.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JulienRineau/diffusion-transformer&#34;&gt;Code Repository&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;DiT Architecture&#34; src=&#34;http://localhost:1313/img/dit/architecture.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;implementation-details&#34;&gt;Implementation details&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Firstlayer:&lt;/strong&gt; A Python class that initializes the input processing. It includes a &lt;code&gt;Patchify&lt;/code&gt; module to convert images into patches, a learnable positional embedding, and separate embedding layers for timesteps and class labels. The forward method combines these elements to prepare the input for the DiT blocks.
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b, c, h, w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;patch_size
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Reshape and permute to get patches&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(b, c, h &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; p, p, w &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; p, p)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;permute(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;contiguous()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view(b, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, p &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; c)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Project patches to embedding dimension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; projection(x)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LayerNorm:&lt;/strong&gt; The LayerNorm affine transformation (scaling and shifting) is provided by a &lt;code&gt;adaLN_modulation&lt;/code&gt; module, which computes these parameters based on the conditioning vector. This allows the scaling and shifting to be input-dependent rather than fixed learnable parameters.
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;adaLN_modulation &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequential(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;SiLU(), nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(config&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;n_embd, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; config&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;n_embd, bias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MLP:&lt;/strong&gt; Transformers struggle with spatial coherence in image tasks, often producing &amp;ldquo;patchy&amp;rdquo; outputs. To address this, we add a depth-wise convolution to the FFN layer, as introduced in the &lt;a href=&#34;https://arxiv.org/pdf/2104.05707&#34;&gt;LocalViT paper&lt;/a&gt;. This allows the model to mix information from nearby pixels efficiently, improving spatial awareness and output quality with minimal computational cost.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modulate:&lt;/strong&gt; Performs element-wise modulation of normalized inputs using computed shift and scale parameters. It&amp;rsquo;s used in both DiTBlock and FinalLayer to apply adaptive normalization.
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shift, scale &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;adaLN_modulation(c)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;chunk(&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, dim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; modulate(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;norm_final(x), shift, scale)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SelfAttention:&lt;/strong&gt; Multi-head self-attention module. It uses linear layers to project inputs into query, key, and value representations, applies scaled dot-product attention using PyTorch&amp;rsquo;s functional API, and projects the output back to the original embedding dimension.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FinalLayer:&lt;/strong&gt; It uses a LayerNorm without affine parameters, computes adaptive normalization parameters via the &lt;code&gt;adaLN_modulation&lt;/code&gt; module, and applies a final linear projection to reshape the output to the desired dimensions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;classifier-free-guidance&#34;&gt;Classifier-free guidance&lt;/h3&gt;
\[ \hat{\varepsilon}_\theta(x_t, c) = \varepsilon_\theta(x_t, \emptyset) + s \cdot (\varepsilon_\theta(x_t, c) - \varepsilon_\theta(x_t, \emptyset))\]&lt;p&gt;
Where $c$ is the condition, $\emptyset$ is a null condition, and $s &amp;gt; 1$ is the guidance scale.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Wav2Vec 2.0 Implementation</title>
      <link>http://localhost:1313/posts/wav2vec2/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/wav2vec2/</guid>
      <description>&lt;p&gt;A Wav2Vec 2.0 implementation using PyTorch Lightning. This project aims to create a clean, modifiable building block for speech reco&lt;/p&gt;
&lt;p&gt;gnition research. It uses common tools for optimized training and effective monitoring. The implementation includes code for model training, dataset preparation, and evaluation. This page also details the results of pretraining on the Libri-Speech dataset.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JulienRineau/wav2vec&#34;&gt;Code Repository&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;Wav2Vec Architecture&#34; src=&#34;http://localhost:1313/img/wav2vec2/architechture_paper.png&#34;&gt;
My implementation closely follows the Wav2Vec 2.0 BASE model architecture:&lt;/p&gt;</description>
    </item>
    <item>
      <title>LoRA Instruction Tuning Implementation</title>
      <link>http://localhost:1313/posts/rlhf/</link>
      <pubDate>Thu, 18 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/rlhf/</guid>
      <description>&lt;p&gt;Following my previous GPT-2 pretraining project, this is clean pipeline for fine-tuning with a &amp;ldquo;from scratch&amp;rdquo; implementation of LoRA that can be reused for other project. I&amp;rsquo;m choosing GPT-2 because I&amp;rsquo;m renting a small GPU for the experiment with a small memory but this can be reproduced with any other transformer model.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JulienRineau/gpt2-workflow&#34;&gt;Code Repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Qwen2 benchmark comparison&#34; src=&#34;http://localhost:1313/img/qwen-rlhf/qwen2-benchmark.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;I used the Alpaca-GPT4 dataset from Stanford that contains 52K instruction-following data generated by GPT-4. Each row of the dataset contains an instruction, an optional input that provide context, and an output:&lt;/p&gt;</description>
    </item>
    <item>
      <title>GPT-2 Pretraining</title>
      <link>http://localhost:1313/posts/gpt-lightning/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/gpt-lightning/</guid>
      <description>&lt;p&gt;A nano-GPT implementation with Pytorch Lightning. The goal is to have a clean building block for other research projects by containing just enough manual implementation do be easily modifiable, but also by using common tools to have a painless optimized training and nice monitoring. Its contains the code to train the model, prepare the dataset and run evals. This page also details results I got training on HF&amp;rsquo;s FineWeb-Edu.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JulienRineau/gpt2-workflow&#34;&gt;Code Repository&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>U-Net for Segmentation</title>
      <link>http://localhost:1313/posts/unet-segmentation/</link>
      <pubDate>Wed, 13 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/unet-segmentation/</guid>
      <description>&lt;p&gt;A simple Pytroch U-Net implementation. The goal is to have an clean building block that can be used in other bigger projects (e.g. Diffusion). The model is tested with a segmentation task on the MIT scene-parse-150 dataset.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JulienRineau/unet-segmentation&#34;&gt;Code Repository&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;U-Net Architecture&#34; src=&#34;http://localhost:1313/img/unet-segmentation/u-net-architecture.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The network is built up as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The network consists of a downsampling path, a bottleneck, and an upsampling path.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the downsampling path:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A sequence of DoubleConv modules are applied. Each DoubleConv consists of two convolutional layers, each followed by batch normalization and GELU activation.&lt;/li&gt;
&lt;li&gt;After each DoubleConv, a max pooling operation is applied to reduce the spatial dimensions.&lt;/li&gt;
&lt;li&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;DoubleConv&lt;/span&gt;(nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, in_channels, out_channedls):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        super(DoubleConv, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;conv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequential(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Conv2d(in_channels, out_channels, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, bias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;BatchNorm2d(out_channels),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;GELU(approximate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tanh&amp;#34;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Conv2d(out_channels, out_channels, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, bias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;GELU(approximate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tanh&amp;#34;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forward&lt;/span&gt;(self, x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;conv(x)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At the bottom of the network, a bottleneck DoubleConv is applied, doubling the number of channels.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL Policy for Legged Locomotion</title>
      <link>http://localhost:1313/posts/rl-policy/</link>
      <pubDate>Sun, 25 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/rl-policy/</guid>
      <description>&lt;p&gt;Quadrupeds robots currently have difficulty overcoming rough terrains, the goal of this project is to improve the agility and robustness of legged locomotion over complex terrain using reinforcement learning. The project consists of implementing the following paper from Nvidia &lt;a href=&#34;https://arxiv.org/pdf/2109.11978&#34; title=&#34;Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning&#34;&gt;Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning&lt;/a&gt; and adapt it to the unitree A1
&lt;img alt=&#34;Unitree A1&#34; src=&#34;http://localhost:1313/img/rl-policy/a1.webp&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;problem-statement&#34;&gt;Problem statement&lt;/h2&gt;
&lt;p&gt;Traditionnaly locomotion is achieve through Optimization algorithms, especially Model Predictive Control (MPC). MPC is a feedback control algorithm that uses a system model to predict future behavior and optimize control inputs for desired trajectories. For robots, such as in MIT&amp;rsquo;s Cheetah 3 project (&lt;a href=&#34;https://dspace.mit.edu/bitstream/handle/1721.1/138000/convex_mpc_2fix.pdf&#34; title=&#34;Dynamic Locomotion in the MIT Cheetah 3 Through Convex Model-Predictive Contro&#34;&gt;Dynamic Locomotion in the MIT Cheetah 3 Through Convex Model-Predictive Contro&lt;/a&gt;) the control task is framed as a convex optimization problem to minimize trajectory errors and control efforts within friction constraints.&lt;/p&gt;</description>
    </item>
    <item>
      <title>MPC Ball Balancing Robot</title>
      <link>http://localhost:1313/posts/ball-balancing-robot/</link>
      <pubDate>Sat, 25 Mar 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/ball-balancing-robot/</guid>
      <description>&lt;p&gt;Control a ball on a plate using a robotic manipulator and a MPC controller. This project was carried out as part of the CS206B Robotic Manipulation and Interaction course at UC Berkeley&lt;/p&gt;
&lt;h2 id=&#34;mpc-controller&#34;&gt;MPC Controller&lt;/h2&gt;
&lt;p&gt;MPC is a type of feedback control algorithm, in which a model of the system is used to make predictions about it&amp;rsquo;s future behavior. The control inputs are then computed based on the predictions, with the goal of achieving the desired system behavior. This is done iteratively, with the model being updated at each time step based on the measured system outputs.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
