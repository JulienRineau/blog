<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Recipe on Julien Rineau</title>
    <link>http://localhost:1313/categories/recipe/</link>
    <description>Recent content in Recipe on Julien Rineau</description>
    <generator>Hugo -- 0.127.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 May 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/recipe/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Wav2Vec 2.0 Implementation</title>
      <link>http://localhost:1313/posts/wav2vec2/</link>
      <pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/wav2vec2/</guid>
      <description>A Wav2Vec 2.0 implementation using PyTorch Lightning. This project aims to create a clean, modifiable building block for speech recognition research. It uses common tools for optimized training and effective monitoring. The implementation includes code for model training, dataset preparation, and evaluation. This page also details the results of pretraining on the LibriSpeech dataset. Code Repository
Architecture My implementation closely follows the Wav2Vec 2.0 BASE model architecture:
768 embedding size 8 attention heads 12 transformer blocks 512 convolutional channels in the feature encoder 2 groups and 320 choices per group in the quantizer This configuration results in approximately 95M parameters.</description>
    </item>
    <item>
      <title>GPT-2 Pretraining Implementation</title>
      <link>http://localhost:1313/posts/gpt-lightning/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/gpt-lightning/</guid>
      <description>A nano-GPT implementation with Pytorch Lightning. The goal is to have a clean building block for other research projects by containing just enough manual implementation do be easily modifiable, but also by using common tools to have a painless optimized training and nice monitoring. Its contains the code to train the model, prepare the dataset and run evals. This page also details results I got training on HF&amp;rsquo;s FineWeb-Edu.
Code Repository</description>
    </item>
  </channel>
</rss>
