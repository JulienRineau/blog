<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Recipe on Julien Rineau</title>
    <link>http://localhost:1313/categories/recipe/</link>
    <description>Recent content in Recipe on Julien Rineau</description>
    <generator>Hugo -- 0.141.0</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Mar 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/recipe/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Wav2Vec 2.0 Implementation</title>
      <link>http://localhost:1313/posts/wav2vec2/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/wav2vec2/</guid>
      <description>&lt;p&gt;A Wav2Vec 2.0 implementation using PyTorch Lightning. This project aims to create a clean, modifiable building block for speech reco&lt;/p&gt;
&lt;p&gt;gnition research. It uses common tools for optimized training and effective monitoring. The implementation includes code for model training, dataset preparation, and evaluation. This page also details the results of pretraining on the Libri-Speech dataset.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JulienRineau/wav2vec&#34;&gt;Code Repository&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;Wav2Vec Architecture&#34; src=&#34;http://localhost:1313/img/wav2vec2/architechture_paper.png&#34;&gt;
My implementation closely follows the Wav2Vec 2.0 BASE model architecture:&lt;/p&gt;</description>
    </item>
    <item>
      <title>GPT-2 Pretraining</title>
      <link>http://localhost:1313/posts/gpt-lightning/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/gpt-lightning/</guid>
      <description>&lt;p&gt;A nano-GPT implementation with Pytorch Lightning. The goal is to have a clean building block for other research projects by containing just enough manual implementation do be easily modifiable, but also by using common tools to have a painless optimized training and nice monitoring. Its contains the code to train the model, prepare the dataset and run evals. This page also details results I got training on HF&amp;rsquo;s FineWeb-Edu.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JulienRineau/gpt2-workflow&#34;&gt;Code Repository&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
